
---

# ğŸ“˜ **Ridge Regression with Cross-Validated Pipeline (RidgeCV)**

A complete end-to-end machine learning project demonstrating **Ridge Regression**, **Regularization**, **Pipeline Modeling**, **Cross-Validation**, and **Hyperparameter Tuning** using `RidgeCV` in Scikit-Learn.

---

## ğŸ† **Project Overview**

This project implements a **Ridge Regression** model wrapped inside a **Scikit-Learn Pipeline**, combined with **10-fold Cross-Validation** to automatically select the **best value of the regularization parameter (alpha / Î»)**.

The goal of the project is to provide:

* ğŸ” A clean understanding of **why regularization is needed**
* ğŸ— A well-structured ML pipeline (Preprocessing + Model)
* ğŸ§ª Automatic hyperparameter tuning using **RidgeCV**
* ğŸ“‰ Evaluation using **MSE, RÂ²**, and visualization
* ğŸ“Š Business-level insights and discussion

This repository is ideal for:

* Students
* ML beginners
* Data science interview preparation
* Portfolio projects

---

## ğŸ“‚ **Repository Structure**

```
â”‚â”€â”€ ğŸ“ Project Report & PPT
â”‚     â”œâ”€â”€ PROJECT_Ridge_Regression PPT.pdf
â”‚     â”œâ”€â”€ Project Report_ Predicting Housing Prices using Ridge Regression.pdf
â”‚     â”œâ”€â”€ ridge.pdf
â”‚â”€â”€ ğŸ“ Ridge_Pic
â”‚     â”œâ”€â”€ Ridge_Info.png
â”‚     â”œâ”€â”€ Ridge_point.webp
â”‚     â”œâ”€â”€ reg_intro.webp
â”‚â”€â”€ HousingData.csv
â”‚â”€â”€ ridge-regression-on-boston-housing-an-end-to-end.ipynb
â”‚â”€â”€ README.md 

```

---

# ğŸ§  **1. Introduction to Regression & Regularization**

### ğŸ“Œ What is Regression?

Regression is a supervised learning technique used to predict continuous outcomes.
It tries to find the best-fit linear relationship between **features (X)** and **target (y)**.

### ğŸ“Œ Why Regularization?

Real-world data often contains:

* High correlation between features
* Noise
* Multicollinearity
* High variance

â¡ This leads to **overfitting**, where the model performs well on training data but poorly on test data.

### ğŸ©¹ Regularization fixes this.

Regularization introduces a **penalty term** to control model complexity.

---

# ğŸ–¼ **Regression Overview**

<img src="Ridge_Pic/reg_intro.webp" width="700">

---

# ğŸ§© **2. Understanding Ridge Regression (L2 Regularization)**

Ridge Regression adds an **L2 penalty** to shrink coefficients:

[
\text{Loss} = \sum (y - \hat{y})^{2} + \lambda \sum \beta^{2}
]

Where:

* **Î» (alpha)** controls the amount of shrinkage:

  * Î» = 0 â†’ same as Linear Regression
  * High Î» â†’ coefficients shrink more, reducing overfitting

---

# ğŸ–¼ Ridge Regression Concept

<img src="Ridge_Pic/Ridge_Info.png" width="700">

---

# ğŸ¯ **3. RidgeCV: Automatically Finding the Best Î» (alpha)**

Instead of manually selecting alpha, we use:

```python
RidgeCV(alphas=alphas, scoring='neg_mean_squared_error', cv=10)
```

This performs:

* 10-fold cross-validation
* Testing multiple Î± values
* Selecting Î± that minimizes MSE

---

# ğŸ›  **4. End-to-End Pipeline Implementation**

### âœ” Why Pipeline?

A pipeline ensures:

* No data leakage
* Consistent preprocessing
* Clean model structure

---

## ğŸ§ª **Code Used in This Project**

### âœ” Ridge Regression Pipeline with Cross-Validation

```python
ridge_pipeline = Pipeline([
    ('preproc', preprocessor),
    ('model', RidgeCV(alphas=alphas, scoring='neg_mean_squared_error', cv=10))
])

ridge_pipeline.fit(X_train, y_train)

best_alpha = ridge_pipeline.named_steps['model'].alpha_
print("Best alpha:", best_alpha)
```

### âœ” What this does:

* Applies preprocessing
* Trains Ridge model on multiple Î±
* Performs 10-fold CV
* Returns the **optimal Î±**

---

# ğŸ–¼ Ridge Regression Key Points

<img src="Ridge_Pic/Ridge_point.webp" width="700">

---

# ğŸ“Š **5. Model Evaluation & Performance**

After fitting:

* Extract best alpha
* Predict on train & test
* Evaluate using:

  * Mean Squared Error (MSE)
  * Root MSE (RMSE)
  * RÂ² Score

### Example:

```python
y_pred = ridge_pipeline.predict(X_test)

from sklearn.metrics import mean_squared_error, r2_score

print("MSE:", mean_squared_error(y_test, y_pred))
print("R2 Score:", r2_score(y_test, y_pred))
```

---

# ğŸ“ˆ **6. Insights & Interpretation**

### âœ” Best alpha chosen by RidgeCV

Helps prevent overfitting while maintaining good accuracy.

### âœ” Coefficient shrinkage

Ridge shrinks coefficients smoothly â†’ more stable predictions.

### âœ” Pipeline benefits

Avoids leakage, ensures reproducibility, simplifies deployment.

### âœ” Cross-validation advantage

Ensures model performs well on unseen data.

---

# ğŸ§¾ **7. Key Takeaways**

âœ” Ridge Regression reduces overfitting through L2 penalty
âœ” Cross-validation ensures robust hyperparameter selection
âœ” Pipeline maintains clean workflow
âœ” RidgeCV automatically finds the best Î»
âœ” Excellent for correlated datasets & high-dimensional data

---

# ğŸš€ **8. How to Run This Project**

### Install dependencies:

```bash
pip install -r requirements.txt
```

### Run the notebook:

```bash
jupyter notebook ridge_pipeline.ipynb
```

---

# ğŸ¤ **9. Contributions**

Contributions are welcome!
Feel free to submit:

* Pull Requests
* Issues
* Improvements
* Visualizations

---

# â­ **10. Support**

If you like this project:

* â­ Star the repository
* ğŸ“¢ Share it
* ğŸ¤ Connect for collaboration

---



# ğŸŒ **11. My Kaggle Notebook**

You can view the full implementation on Kaggle:

<div align="left">

[![Kaggle Badge](https://img.shields.io/badge/Kaggle-View%20Notebook-blue?style=for-the-badge\&logo=kaggle)](https://www.kaggle.com/code/sudiptbiswas1/ridge-regression-on-boston-housing-an-end-to-end/notebook)

</div>

---

# ğŸ”— **12. Connect With Me**

<div align="left">

[![LinkedIn Badge](https://img.shields.io/badge/LinkedIn-Sudipta%20Biswas-blue?style=for-the-badge\&logo=linkedin)](https://www.linkedin.com/in/sudipta-biswas-298915271/)

</div>

---



